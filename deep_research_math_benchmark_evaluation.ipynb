{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Research Technique Evaluation on Math Benchmarks\n",
        "\n",
        "This notebook comprehensively evaluates the **Test-Time Diffusion Deep Researcher (TTD-DR)** algorithm against challenging mathematical benchmarks and compares its performance with state-of-the-art AI models.\n",
        "\n",
        "## Benchmarks Evaluated:\n",
        "1. **FrontierMath** - Advanced mathematics research problems\n",
        "2. **HARP** - Hard Arithmetic Reasoning Problems\n",
        "3. **IMO-bench** - International Mathematical Olympiad problems\n",
        "4. **AIME** - American Invitational Mathematics Examination\n",
        "5. **MATH-500** - High-school competition mathematics\n",
        "\n",
        "## SOTA Models Compared:\n",
        "- ChatGPT (GPT-4/GPT-4.5)\n",
        "- Gemini 2.5 Pro\n",
        "- Claude 4.1 Opus\n",
        "- Grok 4\n",
        "- DeepSeek V3\n",
        "\n",
        "## Evaluation Methodology:\n",
        "- Run Deep Research approach on each benchmark\n",
        "- Compare accuracy, reasoning depth, and computational efficiency\n",
        "- Visualize performance differences\n",
        "- Analyze strengths and weaknesses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q openai datasets huggingface-hub pandas numpy matplotlib seaborn plotly tqdm scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from openai import OpenAI\n",
        "from datasets import load_dataset\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"✓ All imports successful!\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure API endpoints\n",
        "# For Deep Research (using OptiLLM server)\n",
        "OPTILLM_BASE_URL = \"http://localhost:8001/v1\"\n",
        "OPTILLM_API_KEY = \"optillm\"\n",
        "\n",
        "# Initialize OpenAI client for OptiLLM\n",
        "client = OpenAI(api_key=OPTILLM_API_KEY, base_url=OPTILLM_BASE_URL)\n",
        "\n",
        "# Model configuration\n",
        "BASE_MODEL = \"gpt-4o-mini\"  # Change to your preferred model\n",
        "\n",
        "# Evaluation configuration\n",
        "NUM_PROBLEMS_PER_BENCHMARK = 30  # Number of problems to test per benchmark\n",
        "TIMEOUT_SECONDS = 600  # 10 minutes per problem\n",
        "MAX_DEEP_RESEARCH_ITERATIONS = 5\n",
        "MAX_SOURCES = 30\n",
        "\n",
        "# Results directory\n",
        "RESULTS_DIR = \"deep_research_benchmark_results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"✓ Configuration complete!\")\n",
        "print(f\"Results will be saved to: {RESULTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Start: Run the Complete Evaluation\n",
        "\n",
        "The complete evaluation script is available as a standalone Python file. You can run it in two ways:\n",
        "\n",
        "### Option 1: Run as a Python Script\n",
        "```bash\n",
        "cd /Users/wikiwoo/Desktop/optillm\n",
        "python notebooks/deep_research_math_evaluation_complete.py\n",
        "```\n",
        "\n",
        "### Option 2: Execute from this Notebook\n",
        "Run the cell below to execute the complete evaluation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the complete evaluation script\n",
        "%run notebooks/deep_research_math_evaluation_complete.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Evaluation: Step-by-Step\n",
        "\n",
        "If you want to customize the evaluation or run specific benchmarks, use the cells below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the complete evaluation module\n",
        "sys.path.append('/Users/wikiwoo/Desktop/optillm/notebooks')\n",
        "from deep_research_math_evaluation_complete import (\n",
        "    load_aime_dataset,\n",
        "    load_math500_dataset,\n",
        "    load_imo_dataset,\n",
        "    create_synthetic_frontier_math,\n",
        "    create_synthetic_harp,\n",
        "    evaluate_approach_on_benchmark,\n",
        "    get_sota_performance_data\n",
        ")\n",
        "\n",
        "# Load a specific benchmark\n",
        "print(\"Loading AIME 2024 dataset...\")\n",
        "aime_problems = load_aime_dataset(2024, limit=5)\n",
        "\n",
        "print(f\"\\nLoaded {len(aime_problems)} AIME problems\")\n",
        "print(\"\\nSample problem:\")\n",
        "print(f\"ID: {aime_problems[0]['id']}\")\n",
        "print(f\"Problem: {aime_problems[0]['problem'][:200]}...\")\n",
        "print(f\"Answer: {aime_problems[0]['answer']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate a single approach on a single benchmark\n",
        "# This will take a few minutes depending on the number of problems\n",
        "\n",
        "print(\"Running evaluation of baseline approach on AIME...\")\n",
        "baseline_results = evaluate_approach_on_benchmark(\n",
        "    aime_problems[:3],  # Evaluate only 3 problems for quick demo\n",
        "    approach=\"none\",\n",
        "    benchmark_name=\"AIME-2024-Demo\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Baseline Results Summary:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy: {baseline_results['accuracy']:.1f}%\")\n",
        "print(f\"Correct: {baseline_results['correct']}/{baseline_results['total_problems']}\")\n",
        "print(f\"Avg Tokens: {baseline_results['avg_tokens']:.0f}\")\n",
        "print(f\"Avg Time: {baseline_results['avg_time_seconds']:.1f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now evaluate Deep Research approach\n",
        "print(\"Running evaluation of Deep Research approach on AIME...\")\n",
        "print(\"Note: This will take longer as it performs iterative research\\n\")\n",
        "\n",
        "deep_research_results = evaluate_approach_on_benchmark(\n",
        "    aime_problems[:3],  # Evaluate same 3 problems\n",
        "    approach=\"deep_research\",\n",
        "    benchmark_name=\"AIME-2024-Demo\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Deep Research Results Summary:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy: {deep_research_results['accuracy']:.1f}%\")\n",
        "print(f\"Correct: {deep_research_results['correct']}/{deep_research_results['total_problems']}\")\n",
        "print(f\"Avg Tokens: {deep_research_results['avg_tokens']:.0f}\")\n",
        "print(f\"Avg Time: {deep_research_results['avg_time_seconds']:.1f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare the two approaches\n",
        "comparison_data = {\n",
        "    \"Approach\": [\"Baseline\", \"Deep Research\"],\n",
        "    \"Accuracy (%)\": [baseline_results['accuracy'], deep_research_results['accuracy']],\n",
        "    \"Avg Tokens\": [baseline_results['avg_tokens'], deep_research_results['avg_tokens']],\n",
        "    \"Avg Time (s)\": [baseline_results['avg_time_seconds'], deep_research_results['avg_time_seconds']]\n",
        "}\n",
        "\n",
        "comparison_df_demo = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Side-by-Side Comparison:\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df_demo.to_string(index=False))\n",
        "\n",
        "# Quick visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].bar(comparison_df_demo[\"Approach\"], comparison_df_demo[\"Accuracy (%)\"], color=['#3498db', '#e74c3c'])\n",
        "axes[0].set_ylabel(\"Accuracy (%)\")\n",
        "axes[0].set_title(\"Accuracy Comparison\")\n",
        "axes[0].set_ylim(0, 100)\n",
        "\n",
        "# Tokens\n",
        "axes[1].bar(comparison_df_demo[\"Approach\"], comparison_df_demo[\"Avg Tokens\"], color=['#3498db', '#e74c3c'])\n",
        "axes[1].set_ylabel(\"Average Tokens\")\n",
        "axes[1].set_title(\"Token Usage Comparison\")\n",
        "\n",
        "# Time\n",
        "axes[2].bar(comparison_df_demo[\"Approach\"], comparison_df_demo[\"Avg Time (s)\"], color=['#3498db', '#e74c3c'])\n",
        "axes[2].set_ylabel(\"Average Time (s)\")\n",
        "axes[2].set_title(\"Time Comparison\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, \"demo_comparison.png\"), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✓ Visualization saved to {RESULTS_DIR}/demo_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SOTA Performance Comparison\n",
        "\n",
        "Let's compare Deep Research with state-of-the-art models across all benchmarks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SOTA performance data\n",
        "sota_df = get_sota_performance_data()\n",
        "\n",
        "print(\"SOTA Model Performance (Published Benchmarks):\")\n",
        "print(\"=\"*80)\n",
        "display(sota_df)\n",
        "\n",
        "# Create a comprehensive comparison visualization\n",
        "benchmarks = [\"AIME-2024\", \"MATH-500\", \"IMO\", \"FrontierMath\", \"HARP\"]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for benchmark in benchmarks:\n",
        "    if benchmark in sota_df.columns:\n",
        "        fig.add_trace(go.Bar(\n",
        "            name=benchmark,\n",
        "            x=sota_df[\"Model\"],\n",
        "            y=sota_df[benchmark],\n",
        "            text=sota_df[benchmark].round(1),\n",
        "            textposition='auto',\n",
        "        ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"SOTA Models Performance Across Math Benchmarks\",\n",
        "    xaxis_title=\"Model\",\n",
        "    yaxis_title=\"Accuracy (%)\",\n",
        "    barmode=\"group\",\n",
        "    height=600,\n",
        "    xaxis_tickangle=-45,\n",
        "    font=dict(size=12),\n",
        "    legend=dict(\n",
        "        orientation=\"h\",\n",
        "        yanchor=\"bottom\",\n",
        "        y=1.02,\n",
        "        xanchor=\"right\",\n",
        "        x=1\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Save\n",
        "fig.write_html(os.path.join(RESULTS_DIR, \"sota_baseline.html\"))\n",
        "print(f\"\\n✓ Saved SOTA baseline visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion and Key Findings\n",
        "\n",
        "### Summary\n",
        "\n",
        "This evaluation provides comprehensive insights into the Deep Research (TTD-DR) technique's performance on mathematical reasoning tasks compared to SOTA models.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Iterative Refinement**: Deep Research uses Test-Time Diffusion with iterative denoising\n",
        "2. **External Knowledge**: Integrates web search for enhanced reasoning\n",
        "3. **Draft-Guided Search**: Identifies knowledge gaps and performs targeted retrieval\n",
        "4. **Quality Assessment**: Automatically evaluates draft quality for termination\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "- **Pros**: More comprehensive reasoning, access to external knowledge, systematic gap analysis\n",
        "- **Cons**: Higher token usage, longer inference time\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "Based on your specific use case:\n",
        "- **When to use Deep Research**: Complex problems requiring multi-step reasoning and external knowledge\n",
        "- **When to use baseline**: Simple problems where quick responses are needed\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Run the full evaluation with `NUM_PROBLEMS_PER_BENCHMARK = 30`\n",
        "2. Test with different base models (GPT-4, Claude, etc.)\n",
        "3. Tune `MAX_DEEP_RESEARCH_ITERATIONS` and `MAX_SOURCES` parameters\n",
        "4. Analyze error patterns for specific problem types\n",
        "\n",
        "### Files Generated\n",
        "\n",
        "All results are saved in `deep_research_benchmark_results/`:\n",
        "- `summary_report.json` - Complete evaluation data\n",
        "- `*_results.json` - Per-benchmark detailed results\n",
        "- `*.html` - Interactive visualizations\n",
        "- `*.png` - Static charts\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
